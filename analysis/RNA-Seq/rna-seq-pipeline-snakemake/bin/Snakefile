#! /usr/bin/env bash
## Snakefile
####################
import glob
import re

BASE_DIR = workflow.basedir + "/../"
#suffix for single-end reads
SE = config["SE_SUFFIX"]
#suffix for paired-end reads
R1 = config["R1_SUFFIX"]
R2 = config["R2_SUFFIX"]

#R2 = R1.replace("1","2")
fastqs_p1 = glob.glob("fastq/*"+R1+"*")
# sort the fastqs in order the R1 and R2 files to match each othter. 
# fastqs_p1.sort()
#fastqs_p2 = glob.glob("fastq/*"+R2+"*")
#fastqs_p2.sort()
fastqs_p2 = [re.sub("(fastq/.+)"+ R1 +"(.*)$", "\\1"+ R2 +"\\2",a) for a in fastqs_p1]
print(fastqs_p1)
print(fastqs_p2)
fastqs_se = glob.glob("fastq/*"+SE+"*")
fastqs_se = [x for x in fastqs_se if x not in fastqs_p1 and x not in fastqs_p2 ]

SAMPLES_SE = [re.sub("fastq/(.+)"+ SE +".*$","\\1",a) for a in fastqs_se]
SAMPLES_PE = [re.sub("fastq/(.+)"+ R1 +".*$","\\1",a) for a in fastqs_p1]
#sorting causes problems.
#SAMPLES_PE.sort()
FASTQ_DICT = dict(zip(SAMPLES_SE,fastqs_se))
for idx in range(len(fastqs_p1)):
  FASTQ_DICT[SAMPLES_PE[idx]] = [fastqs_p1[idx],fastqs_p2[idx]]

SAMPLES = SAMPLES_SE + SAMPLES_PE

## annotation path
GENOME = config["GENOME"]
STAR_INDEX = config["STAR_INDEX_PATH"]+GENOME

GTF_DICT = {"hg38":"hg38.gencode.v38.annotation.gtf",
            "hg19":"hg19.gencode.v19.annotation.gtf",
            "mm10":"mm10.gencode.vM25.annotation.gtf",
            "mm9":"mm9.gencode.vM1.annotation.gtf",
            "panTro6":"panTro6.gtf",
            "calJac3":"calJac3.gtf",
            "panPan2":"panPan2.gtf"
            }

GTF = GTF_DICT[GENOME]
# dependencies.
MARKDUP=BASE_DIR+"dependencies/picard.jar MarkDuplicates"

print(SAMPLES)
print(FASTQ_DICT)

rule all:
  input: 
    expand("featureCounts/{sample}.counts",sample=SAMPLES),
    expand("rpkm/{sample}-chrM.rpkm",sample=SAMPLES),
    "combined-chrM.rpkm",
    "combined-chrM.counts",
    expand("bigWig/{sample}.sorted.bw",sample=SAMPLES),
    "all_sample.qc.txt"

rule fastp: 
  output: 
    "fastp_out/{sample}_R1.clean.fastq.gz",
    "fastp_out/{sample}_R2.clean.fastq.gz", 
    "fastp_out/{sample}.fastp.json", 
    "fastp_out/{sample}.fastp.html"
  input: 
    lambda wildcards: FASTQ_DICT[wildcards.sample]
  run:
    command =  "fastp -i " + str(input[0]) + " -I " + str(input[1]) + " -o " + str(output[0]) + " -O " + \
      str(output[1]) + " -j " + str(output[2]) + " -h " + str(output[3])
    shell(command)

  

rule star_align:
  output: 
    bam1 = temp("STAR_out/{sample}/Aligned.sortedByCoord.out.bam"),
    bam2 = "bam/{sample}.sorted.bam",
    bai = "bam/{sample}.sorted.bam.bai",
    raw_qc = "qc/{sample}.raw.flagstat.qc",
    star_out = "STAR_out/{sample}/Log.final.out"
  input:
  #  lambda wildcards: FASTQ_DICT[wildcards.sample]
    r1 = "fastp_out/{sample}_R1.clean.fastq.gz",
    r2 = "fastp_out/{sample}_R2.clean.fastq.gz",

  threads: 10 
  run:
    fastqs = " ".join(input)
    if fastqs.endswith(".bz2"): 
      format = "bzcat"
    elif fastqs.endswith(".gz"):
        format = "zcat"
    else: 
        format = "cat"
    shell(
#    "STAR --genomeDir {STAR_INDEX} --readFilesIn {fastqs} "
    "STAR --genomeDir {STAR_INDEX} --readFilesIn {input.r1} {input.r2} "
    "--runThreadN {threads} "
    "--outFileNamePrefix STAR_out/{wildcards.sample}/ "
    "--outFilterMultimapScoreRange 1 "
    "--outFilterMultimapNmax 20 "
    "--outFilterMismatchNmax 10 "
    "--alignIntronMax 500000 "
    "--alignMatesGapMax 1000000 "
    "--sjdbScore 2 "
    "--alignSJDBoverhangMin 3 "
    "--genomeLoad NoSharedMemory "
    "--readFilesCommand {format} "
    "--sjdbOverhang 100 "
    "--outSAMstrandField intronMotif "
    "--outSAMtype BAM SortedByCoordinate "
    "--twopassMode Basic ;"
    "ln {output.bam1} {output.bam2};"
    "samtools index {output.bam2};"
		"samtools flagstat {output.bam2} > {output.raw_qc};"
    )

rule bam_rmdup:
  input:
    bam = "bam/{sample}.sorted.bam",
  output:
    bam = "bam/{sample}.nodup.bam",
    bai = "bam/{sample}.nodup.bam.bai",
    qc = "qc/{sample}.dup.qc"
  log:
    "logs/markdup/{sample}.markdup.log"
  threads: 3
  shell:
    "java -Xmx12G -XX:ParallelGCThreads=3 -jar {MARKDUP} TMP_DIR=tmp/{wildcards.sample} INPUT={input.bam} OUTPUT={output.bam} METRICS_FILE={output.qc} VALIDATION_STRINGENCY=LENIENT ASSUME_SORTED=true REMOVE_DUPLICATES=true 2> {log};"
    "samtools index {output.bam}"

rule bam2bigwig:
  input:
    bam = "bam/{sample}.sorted.bam"
  output: 
    bw = "bigWig/{sample}.sorted.bw"
  threads: 6
  shell:
    "bamCoverage -b {input.bam} -o {output.bw} --outFileFormat bigwig "
    "-bs 50 --numberOfProcessors {threads} --normalizeUsing RPKM"


rule feature_count_rpkm:
  input:
    bam ="bam/{sample}.sorted.bam",
    bai = "bam/{sample}.sorted.bam.bai",
  output:
    counts = "featureCounts/{sample}.counts",
    rpkm = "rpkm/{sample}-chrM.rpkm"
  threads: 1
  shell:
    "featureCounts -a {BASE_DIR}/annotation/gtf/{GTF} -o {output.counts} {input.bam} "
    " -F GTF -T {threads} -t exon -g gene_name; "
    "grep -v 'chrM' {output.counts} | Rscript {BASE_DIR}/scripts/featurecounts2rpkm.r - > {output.rpkm}"


rule combine_counts_rpkms:
  input:
    counts = expand("featureCounts/{samples}.counts",samples=SAMPLES),
    rpkms = expand("rpkm/{samples}-chrM.rpkm",samples=SAMPLES),
  output: 
    rpkm = "{sample}.rpkm",
    counts = "{sample}.counts"
  threads: 1
  shell:
    "len=$(ls {input.counts}|wc -l);"
    "paste {input.rpkms} |cut -f 1-6,$(seq -s, 7 7 $((7*len))) > {output.rpkm};"
    "paste {input.counts} |cut -f 1-6,$(seq -s, 7 7 $((7*len))) |"
    "grep -v 'chrM' > {output.counts}"


rule lastqc:
    input:
        raw = expand("STAR_out/{samples}/Log.final.out",samples=SAMPLES),
        dup = expand("qc/{samples}.dup.qc",samples=SAMPLES)
    output:
        "{sample}.qc.txt"
    threads: 1
    run:
      out = open(output[0],'w')
      out.write("\t".join(["Sample","Total","Mapped","Filtered","Uniq","Map%","Dup%"])+"\n")
      for idx in range(len(input.raw)):
        samples = re.match("STAR_out\/(.*)\/Log.final.out",input.raw[idx]).groups()[0]
        raw_file = open(input.raw[idx], 'r')
        for line in raw_file:
          words = line.strip().split('|')
          print(words)
          if words[0] == "Number of input reads ":
              total = words[1]
          elif words[0] == "Uniquely mapped reads number ": 
              uniq_mapped = words[1]
          elif words[0] == "Number of reads mapped to multiple loci ":
              multi_mapped = words[1]
        mapped = str(int(uniq_mapped)+int(multi_mapped))
        raw_file.close()
        dup_file = open(input.dup[idx],'r')
        for line in dup_file:
          words = line.strip().split('\t')
          if words[0] == "Unknown Library":
            filt = words[2]
            duplicates = words[6]
            nodup = str(int(filt)-int(duplicates))
        map_p = "%.2f"%(float(mapped)/float(total))
        dup_p = "%.2f"%(float(duplicates)/float(filt))
        out.write("\t".join([samples,total,mapped,filt,nodup,map_p,dup_p])+"\n")



